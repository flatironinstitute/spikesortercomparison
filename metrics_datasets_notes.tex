\documentclass[10pt]{article}
\textwidth 6.3in
\oddsidemargin=0in
\evensidemargin=0in
\usepackage{graphicx,bm,amssymb,amsmath,amsthm,url}

\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\ben}{\begin{enumerate}}
\newcommand{\een}{\end{enumerate}}
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bea}{\begin{eqnarray}} 
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\ba}{\begin{align}} 
\newcommand{\ea}{\end{align}}
\newcommand{\bse}{\begin{subequations}} 
\newcommand{\ese}{\end{subequations}}
\newcommand{\bc}{\begin{center}}
\newcommand{\ec}{\end{center}}
\newcommand{\bfi}{\begin{figure}}
\newcommand{\efi}{\end{figure}}
\newcommand{\ca}[2]{\caption{#1 \label{#2}}}
\newcommand{\ig}[2]{\includegraphics[#1]{#2}}
\newcommand{\tbox}[1]{{\mbox{\tiny #1}}}
\newcommand{\mbf}[1]{{\mathbf #1}}
\newcommand{\pO}{{\partial\Omega}}
\newcommand{\half}{\mbox{\small $\frac{1}{2}$}}
\newcommand{\vt}[2]{\left[\begin{array}{r}#1\\#2\end{array}\right]} % 2-col-vec
\newcommand{\mt}[4]{\left[\begin{array}{rr}#1&#2\\#3&#4\end{array}\right]} % 2x2
\newcommand{\xx}{\mbf{x}}
\newcommand{\ff}{\mbf{f}}
\newcommand{\yy}{\mbf{y}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\ttt}{{\bm \theta}}
\newcommand{\eps}{\varepsilon}
\newcommand{\eeps}{\bm{\eps}}
\newcommand{\HH}{{\cal H}}
\DeclareMathOperator{\re}{Re}
\DeclareMathOperator{\im}{Im}
\newtheorem{thm}{Theorem}
\newtheorem{cnj}[thm]{Conjecture}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{pro}[thm]{Proposition}
\newtheorem{rmk}[thm]{Remark}

\newcommand{\mar}[1]{{\marginpar{\sffamily{\scriptsize #1}}}}
\newcommand{\ahb}[1]{{\mar{AB:#1}}}          % co-author comments
\newcommand{\jfm}[1]{{\mar{JM:#1}}}


\begin{document}
\title{Proposal on metrics and datasets for spike sorting validation}
\author{Alex Barnett and Jeremy Magland, Flatiron Institute}
\date{\today}
\maketitle
\begin{abstract}
  We sketch and extend some ideas from the discussion at the
  Janelia spike sorting meeting of 3/22/18.
  We lay out some choices for metrics and datasets for a community effort to
  compare and validate the major spike sorting packages.
  There is some discussion and personal opinions.
  This document will be open to suggestions from the electrophysiology community.
\end{abstract}

\section{Background and goals}

We are in the process of hosting at Flatiron a web-based platform where
a variety of spike-sorting packages are run on
a set of community-approved ground-truthed datasets, and their
performance metrics made publicly available online via an interactive
graphical front-end, and probably via an API.

The main goal is an objective accuracy comparison of current
spike sorting codes. This should indicate for the e-phys community
the best code to use, and its expected accuracy (or distribution of
accuracies across units),
both of which may depend on context
(probe type, in/ex vivo, whether you have GPU, etc).

A meta-goal is to gather information about which {\em quality metrics}, meaning
metrics computable without ground-truth data, most closely indicate ground-truth
accuracy when it is available.

Curating and gathering such datasets and metrics is a community effort.
Please give feedback and/or additions to this document.
\ahb{one way to comment is in the margin like this}

\subsection{Abbreviations}

\begin{tabular}{ll}
GT & ground-truth(ed)\\
AC & auto-correlation of firing events for a single sorted unit\\
CC & cross-correlation of firings between different sorted units\\
(BP)CM & (best-permuted) confusion matrix \cite{validspike}\\
\end{tabular}

\subsection{Mathematical symbols}
\label{s:symb}

\begin{tabular}{ll}
  $M$ & number of channels (electrodes)\\
  $N$ & number of time points (samples)\\
  $X$ & $M$-by-$N$ matrix of voltages at each time point, with elements $x_{mn}$\\
  $t_j$ & time (in samples) of the $j$th firing event\\
  $k_j$ & label (ie, unit number or classification) of the $j$th firing event,
  in the range $1,\ldots,K$\\
  $K$ & number of units found by a spike sorter\\
  $Q_{kl}$ & confusion matrix element, equal to the
  number of time-matching events labeled $k$ in sorting 1 and $l$ in sorting 2\\
\end{tabular}




\section{Considerations}
\label{s:cons}

One issue is whether to upload {\em sorted data} or {\em algorithms};
the consensus at the meeting seemed to be both.
(Algorithms are harder to upload, but allow the website to show more complete
information such as runtime, RAM usage, etc; they also reduce the potential
for hand-tweaking of results.)
We have already wrapped several popular algorithms
to run in the MountainLab framework, and will continue in this
mode initially.
We can run algorithms from a github repo with a dockerfile, which
could become a standard submission format.
Being careful about version numbers will be important as groups
start to update their algorithms.

Held-out or {\em hidden} data in the style of the NetFlix Challenge might
be useful.
We are not yet planning to do this.

All algorithms benchmarked have to be {\em fully automatic},
come with some {\em meta-data} such the parameter set, version number, etc.
Algorithms should be cleanly separated from any visualization/curation/GUI tools.
Any curation or post-processing (eg based on their internal quality metrics)
should be automatic and included with the algorithm.

We decided to avoid the computation of confusion matrices for the accuracy
vs GT; they are not relevant unless there are a large number of units to match.
% jfm

\subsection{Interface and data formats}
\label{s:int}

One cannot proceed without a
standard {\em interface} between algorithms and e-phys datasets.
We suggest that the interface be (see notation in Sec.~\ref{s:symb}):

\vspace{2ex}

\begin{minipage}[t]{1in}Inputs:\end{minipage} \begin{minipage}[t]{5in}
  $X$: Raw (preferably unfiltered)
  voltage recording, as a matrix of size $M$ (number of channels) by
  $T$ (number of
    time-points), stored in binary format without any header.
  An example is 16-bit signed integer format
  with all channels stored
  contiguously for each timepoint (ie column-major order).
  Since some available data is already filtered, we must also allow
  filtered data in the same format.

  Accompanying parameter file(s): giving $M$, $N$,
  the sample rate (samples per second), and 2D electrode coordinates in $\mu$m.
  \end{minipage}

\begin{minipage}[t]{1in}Output:\end{minipage} \begin{minipage}[t]{5in}
  Lists of firing times $t_j$ and labels (unit assignments) $k_j$
  for the found events $j=1,\ldots,N$. Time is either in seconds or in
  time samples of the input.
  This can take the form of a 2-by-$N$ (number of events found) matrix.
  Events needn't be ordered in time.
\end{minipage}

\vspace{2ex}

The advantage of this simple output format
is that no inner workings of algorithms are touched;
there are algorithms such as ICA that have no pipeline stages
in common with other algorithms. (Benchmarking sub-stages is a separate
task that we do not tackle here.)
Writing wrappers and format-converters is easy, and should not
slow down the sorter unless the (large) input data is converted.
All quantities of interest to display in a web-based exploration
of the sorting result (peak channel, mean waveform, etc)
can be derived from the union of above input and output data;
some of these are expensive but can be computed after an algorithm runs and cached.

A disadvantage of the interface is that it does not allow for
probabilistic outputs (that were discussed at the meeting); however,
there are currently few if any spike sorters that produce such
outputs. In addition, a viable patch to such a probabilistic output is
to draw $\sim 20$ independent samples from it and compute all metrics for
each sample, giving distributions of each metric.


\section{Datasets}

It was agreed that a variety of brain regions, recording conditions, probes, and types of data are needed.
Here are some datasets, most of which were discussed at the meeting.
They fall into three categories.

\ben
\item {\bf Recordings with ground-truth.}
  
  Good features: gold-standard.
  Bad features: very small sample size,
  not in awake animals or various regions yet.
\footnote{It was noted that there are no ground-truthed recordings in
  awake, let alone behaving, animals.
  Please correct me if I got this wrong.}

  \bi
\item Neto, Kampff et al '16. \cite{neto16}
  32-channel and 128-channel with single juxtacellular GT, in vivo rat,
  anesthetized, motor, sensory or parietal cortex, 30 kHz, roughly 10 min long.
  \url{http://www.kampff-lab.org/validating-electrodes/}
  
  We propose to include:

  \verb+2014-11-25_Pair 3.0+ : 32-channel, GT 52 $\mu$m from electrode,
  $N_\tbox{true} = 347$.

  \verb+2015_09_03_Cell.9.0+ : 128-channel, GT 29 $\mu$m from electrode
  (this is a bursting pair as discussed in \cite{mountainsort}),
  $N_\tbox{true} = 4895$.

  These appear to be the only datasets where the GT unit can be viably sorted.
  The GT units are very easy to sort, hence this might not be useful
  in differentiating algorithms.
  % The third to try would be 2014_03_26_Cell2.0 but I recall it being unsortable.
  
\item Yger et al. '18. \cite{yger18}
  252-channel ($16\times 16$ array, 30 $\mu$m),
  w/ loose patch, in vitro, mouse retina.
  Around 20 recordings of typical length 5 min, each with one GT unit
  with typically 500-5000 firings. Varying SNR of the GT units.
\ahb{The paper discusses at least 37 neurons; unsure how many GT are available.}
  
  \url{http://www.yger.net/software/ground-truth-recordings/}
% Password: gt252.
  
  Subset at \url{https://dio.org/10.5281/zenodo.1205233}

  To do: decide if all or a subset of datasets should be included.
  
  \item Franke et al '15 \cite{frankeoverlap}.
    Tetrode with {\em dual} patch-clamp GT units, ex vivo rat cortex.
    33.3kHz.
    Includes 25000 spikes with overlaps of less than 1.5 ms induced by
    current injections, designed to test overlapping (colliding) spikes.
    To do: write to authors and ask for data.
    
  \item Boyden, possibly (James?)

  \item simultaneous calcium imaging and e-phys (Shepard lab)? Provides low
    time accuracy, but for larger number of units.

\item
Brendon Watson's recent thread at 
\url{https://groups.google.com/forum/#!topic/klustaviewas/pfVC-CMSCTs}
mentions upcoming data from Dan English.
\ei
  
\item {\bf Hydrid datasets.}
  
  Good features: they embody the correct noise model.
  
  Bad features: biased towards already-sorted (eg high firing rate) units.

  \bi
\item Steinmetz, Harris et al, 2016.
  \url{http://phy.cortexlab.net/data/sortingComparison/datasets/}
\item A hybrid dataset can be created by adding additional firings of
  sorted units to any dataset
  (this idea is also described as the ``spike addition metric'' in
  \cite{validspike}).

  \ei

\item {\bf Recordings without ground truth.}

  Good features: abundant, possible to create partial GT by hiding a subset of electrodes.

  Bad features: no GT.
  
  \bi
\item Litke, Chichilnisky et al, 2004 \cite{litke}. 512-channel,
  hexagonal array, at least 2 hrs available.
  monkey retina, in vitro, 20 kHz.
  Used in YASS testing; have to check how much can be public.
  
\item Josh Siegle (Allen Institute), 2018. Six simultaneous neuropixels probes
  each with $\sim150$ channels within the brain. Length unknown.

\item Tetrodes and
  18-channel polymer probes from Jason Chung et al \cite{mountainsort}.
  The tetrodes are hippocampal in behaving rats, and have three independent
  human sortings available as a vague GT.
  To do: check if data can be released.
  
\ei
  

  \item {\bf Simulated (in-silico) datasets.}
  
    Good features: abundant number of GT units,
    allows arbitrary electrode design and drift
    simulation.

  Bad features: unvalidated themselves,
  noise models too clean, no artifacts, no detailed modeling
  of electrode effects, no non-rigid drift.

  \bi
\item BioNET (Allen Institute) simulations, by C. Mitelut.

  We suggest a single non-drifting and a single drifting dataset, with
  neuropixels probe geometry. Exist as 4-minute segments.
  
  These are expensive to run.
  
\item {\tt ViSAPy} simulations, Hagen et al 2015 \cite{visapy}.

  The repo contains scripts for 16-channel polytrodes, etc.
  
  \url{https://github.com/espenhgn/ViSAPy}

  Yger et al \cite{yger18} uses this code but doesn't report data.
  We have not yet used it.

  \ei
  
\een

Notes:
\ben
\item
Retinal data is quite different from cortical: retinal has
much lower noise, almost no drift, partial validation by planar coverage of
certain cell types (eg ON/OFF parasol), and axonal spikes that are non-local
(propagate across the entire array).
\item
We exclude other well-known datasets (eg Harris et al
tetrode from 2000, Martinez et al synthetic from 2009, Cmunas-Mesa et
al Neurocube simulator from 2013) that are either too small or have
been superceded.
\een


\section{Metrics}

We list metrics that could be reported for each algorithm-dataset pair.
Most are just sketched, pending formulae, then scripts that implement them.
A possible classification of metrics is as follows.
The three classes I, II, and III have descending order of
confidence. Within a class no rank ordering is implied.
We first give an extensive list; below we propose a concrete subset.

\bi
\item {\bf Accuracy vs ground-truth} (Class I).

  We first choose $\tau \approx 1 $ms as the allowable spike time error;
  we have not found its choice makes much difference (timing only becomes an
  issue at the 0.1 ms or less level).
  
  GT should be in the form of a vector of firing times and labels. Usually
    human work is needed to extract this set by thresholding eg an intracellular or juxtacellular recording; often there is a subjective choice of threshold.
  
  \ben
\item Inaccuracy.
  For a given GT unit, the smallest value over the
  sorted units of the ratio: number of spikes missed over true number of spikes.
  Is in range $[0,1]$, with zero best.
  
\item False positive fraction. 
For a given GT unit, the smallest value over the
sorted units of the ratio: number of spikes not matching GT over number of sorted spikes.
  Is in range $[0,1]$, with zero best.

\item Overall error rate. This combines the above: number of missed plus number
  of false positives, all divided by the union of true and sorted spikes.
  This metric is similar but not identical to $f_k$ in \cite{validspike}.
  Is in range $[0,1]$, with zero best.

  \een
\item {\bf Biophysical metrics} (Class II).
  \ben
\item rate of refractory violations (ISI, or AC plots),
  separately for each unit. Requires choice of ISI lower cut-off, eg 2 ms.
  Called $\mbf{f}_1^p$ in \cite{Hill2011}.
\item Existence of unphysical notch in CC. This is useful
  for seeing missed spikes due to collisions; see eg \cite{chaitu}.
  We need to choose a notch width. This could be computed faster than
  the entire set of CCs. Related to $\mbf{f}_3^n$ of \cite{Hill2011}.
\item Highly-one-sided CC, indicating a bursting pair (or triplet, etc),
  that has been mistakenly split.
  Of course, one-sided CCs can occur legitimately due to synaptic coupling;
  we seek expertise on this.
\item Refractory gap in CC between a pair that matches the AC of each in the pair; this indicates a false split, due to eg drift.
\item Disappearance of a unit over time, as an indicator of failure
  to handle drift.
\item
  Validation by spatial localization of place cell
  firings \cite{mountainsort} (special to hippocampus of behaving rodent).
\een
  
\item {\bf Quality metrics} not requiring ground-truth (Class III).
  
  These are ``surrogate'' metrics for quality that
  can be quoted for each unit. We don't yet know which are the best indicators of
  (in)accuracy.
  Crucially, they
  can (and must) all be able to be
  computed from only the input and output data of Section \eqref{s:int}.
  \ben
\item Peak amplitude of mean template, divided by RMS noise level after filtering.
  Ie, peak SNR. Noise level (std dev) is estimated from the median $L_2$-norm of
  clips from the filtered data.
  This requires a standard definition of filter parameters (not including
  spatial whitening).
\item Noise-overlap \cite{mountainsort}. Is cluster-shape agnostic.
  \footnote{This replaced the idea of amplitude histogram separation from detection threshold.}
\item Isolation \cite{mountainsort}. Is cluster-shape agnostic.
\item 1D projections (eg, amplitude histograms) as used in Pachitariu et al \cite{kilosort}
\item Mahalanobis-style estimates of false-pos and false-neg rates
  based on a Gaussian clusters assumption. Requires (re)computing a local
  feature space.
  \item Various other quality metrics from Hill et al \cite{Hill2011}
  \item Various stability metrics requiring reruns of the sorter \cite{validspike}.
\item Community-supplied quality metrics, that could be uploaded automatically as scripts acting on the data\ldots
  \een

\item {\bf Other metrics} and meta-data (Class IV)
  \ben
\item Algorithm run-time in seconds. Runs our framework performs would
  all be on the same machine.
  This may need a CPU-only and CPU+GPU category.
\item Peak RAM usage.
\item Peak disk usage (temporary intermediate files).
\item Subjective user opinions on ease of installation and use.
  \een
  
\ei

Since algorithm comparison is also needed, Class V could be metrics
of similarity of two sorting outputs:
this we propose to be based on the {\em best-permuted confusion matrix} (BPCM)
as described in \cite{validspike,mountainsort}, which is
diagonal if two sortings match.
The website should be able to produce and display graphically
the BPCM between any two
algorithms run on the same data.

\subsection{Initial implementation plan}

In the first released website we will implement from the above list the following.
Class I: 1,2,3.
Class II: 1.
Class III: 1,2,3.
Class IV: 1.

To be debated.



\section{Prior work and influences}

We are influenced by several previous versions of online
comparison tools, including:
\bi
\item \url{http://neurofinder.codeneuro.org/}\\
  J. Freeman's calcium-imaging algorithm comparison.
  Intuitive interface (click and mouse-over for more detail), submission info,
  gitter chatroom, contest, cash prizes.
  We may or may not want the competitive aspect of leaderboard style.
  No data or outputs are visible, just scores.
\item \url{http://spikefinder.codeneuro.org/}\\
  P. Berens, spikes from calcium fluorescence curves, similar to above.
\item \url{http://spike.g-node.org/}\\
  In this now-defunct
  2011-2012 project the user
  uploads sorted data, which is compared against a {\em hidden} ground truth sorting and optionally published. Layout is a little non-obvious.
  Tags and owners of algorithms are good.
\item \url{http://phy.cortexlab.net/data/sortingComparison/}\\
  N. Steinmetz comparison of several algorithms on hybrid data.
\item \url{http://www.spikesortingtest.com/}\\
  C. Mitelut's comparison site. Good collection of recent
  simulated data, but
  no algorithms, not yet used by others. Metrics: purity and completeness.
\item \url{http://simonster.github.io/SpikeSortingSoftware/}\\
  Large but incomplete list of codes and their features.
\ei

\section{Web and graphical interface}

To do: describe web interface and which summary plots can be brought up.


\section{To do}

Decide whether filtered data is accessible, or if filtering is always
taken to be internal to an algorithm.

Write formulae, then scripts to compute the initial quality metrics from only
$X$, $\{t_j\}$, $\{k_j\}$ above.

Go through datasets and be more specific about which to include, and their
length and size (GB).

Ask re Franke data.

Quantify and collect the simulated datasets.


\bibliographystyle{abbrv}
\bibliography{spikesorting}

\end{document}
